{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer  TfidfTransformer\n",
    "create my bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 2)\t2\n",
      "  (1, 1)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 2)\t1\n",
      "{u'much': 3, u'love': 2, u'her': 1, u'so': 4, u'you': 5, u'hate': 0}\n",
      "3\n",
      "\n",
      "  (0, 2)\t0.668501460176\n",
      "  (0, 5)\t0.334250730088\n",
      "  (0, 4)\t0.469777738499\n",
      "  (0, 3)\t0.469777738499\n",
      "  (1, 2)\t0.40993714596\n",
      "  (1, 5)\t0.40993714596\n",
      "  (1, 0)\t0.576152355165\n",
      "  (1, 1)\t0.576152355165\n",
      "\n",
      "[[ 0.          0.          0.66850146  0.46977774  0.46977774  0.33425073]\n",
      " [ 0.57615236  0.57615236  0.40993715  0.          0.          0.40993715]]\n",
      "2 6\n",
      "\n",
      "\n",
      "now print the word weights of 0 document in the corpus\n",
      "hate 0.0\n",
      "her 0.0\n",
      "love 0.668501460176\n",
      "much 0.469777738499\n",
      "so 0.469777738499\n",
      "you 0.334250730088\n",
      "\n",
      "\n",
      "now print the word weights of 1 document in the corpus\n",
      "hate 0.576152355165\n",
      "her 0.576152355165\n",
      "love 0.40993714596\n",
      "much 0.0\n",
      "so 0.0\n",
      "you 0.40993714596\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "vectorizer=CountVectorizer()\n",
    "#该类会将文本中的词语转换为词频矩阵，矩阵元素a[i][j] 表示j词在i类文本下的词频  \n",
    "transformer=TfidfTransformer()\n",
    "#该类会统计每个词语的tf-idf权值 \n",
    "\n",
    "string1='i love love you so much'\n",
    "string2='i hate you, love her'\n",
    "\n",
    "my_corpus=[string1,string2]   #create corpus with different documents in it\n",
    "#第一个fit_transform是计算tf-idf\n",
    "#第二个fit_transform是将文本转为词频矩阵  \n",
    "bag_of_words=vectorizer.fit_transform(my_corpus)\n",
    "tfidf=transformer.fit_transform(bag_of_words)\n",
    "weight=tfidf.toarray()\n",
    "#将tf-idf矩阵抽取出来，元素a[i][j]表示j词在i类文本中的tf-idf权重  \n",
    "\n",
    "print bag_of_words\n",
    "print vectorizer.vocabulary_ #取词袋模型中的所有词语,及出现次数\n",
    "word=vectorizer.get_feature_names()#获取词袋模型中的所有词语\n",
    "print vectorizer.vocabulary_.get('much')\n",
    "print ''\n",
    "print tfidf\n",
    "print ''\n",
    "print weight\n",
    "print len(weight),len(weight[0])\n",
    "\n",
    "print '\\n'\n",
    "for i in range(len(weight)):\n",
    "    print 'now print the word weights of {} document in the corpus'.format(i)\n",
    "    for j in range(len(weight[0])):\n",
    "        print word[j],weight[i][j]\n",
    "    print '\\n'    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfVectorizer_set with stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1.0\n",
      "  (1, 1)\t0.579738671538\n",
      "  (1, 0)\t0.814802474667\n",
      "\n",
      "[u'hate', u'love']\n",
      "{u'love': 1, u'hate': 0}\n",
      " \n",
      "[ 1.40546511  1.        ]\n",
      "[[ 0.          1.        ]\n",
      " [ 0.81480247  0.57973867]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer=TfidfVectorizer(stop_words='english')   \n",
    "#remove english stop words using sklearn's stop word list \n",
    "string1='i love love you so much'\n",
    "string2='i hate you, love her'\n",
    "lis=[string1,string2]\n",
    "bag_of_words=vectorizer.fit_transform(lis)\n",
    "word=vectorizer.get_feature_names()\n",
    "print bag_of_words\n",
    "\n",
    "print ''\n",
    "print word\n",
    "print vectorizer.vocabulary_\n",
    "print ' '\n",
    "print vectorizer.idf_\n",
    "print bag_of_words.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'henry,', 'I', 'computer', 'scientist']\n",
      "My name henry, I computer scientist\n",
      "M n  nw, I   cpuer cen\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw=stopwords.words('english')\n",
    "\n",
    "#print sw[1:6]\n",
    "#print len(sw)\n",
    "#print u'have' in sw\n",
    "#print u'is' in sw\n",
    "\n",
    "\n",
    "##delete the stopwords in a string\n",
    "test_words1=\"My name is henry, I am a computer scientist\"\n",
    "test_words1=test_words1.split(' ')\n",
    "for word in sw:\n",
    "    while word in test_words1:\n",
    "        test_words1.remove(word)\n",
    "print test_words1\n",
    "test_words1Str=' '.join(test_words1)\n",
    "print test_words1Str\n",
    "\n",
    "test_words2=\"My name is andrew, I am a computer scientist\"\n",
    "for word in sw:\n",
    "    test_words2=test_words2.replace(word,'')\n",
    "print test_words2   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have\n",
      "respons\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer1=SnowballStemmer('english')\n",
    "stemmer2=SnowballStemmer('english',ignore_stopwords=True)\n",
    "print stemmer1.stem('having')\n",
    "print stemmer2.stem('responsiveness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'caress', u'fli', u'die', u'mule', u'deni', u'die', u'agre', u'own', u'humbl', u'size', u'meet', u'state', u'siez', u'item', u'sensat', u'tradit', u'refer', u'colon', u'plot'] \n",
      "[u'caress', u'fli', u'die', u'mule', u'deni', u'die', u'agre', u'own', u'humbl', u'size', u'meet', u'state', u'siez', u'item', u'sensat', u'tradit', u'refer', u'colon', u'plot']\n",
      "caress,fli,die,mule,deni,die,agre,own,humbl,size,meet,state,siez,item,sensat,tradit,refer,colon,plot\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmerP=PorterStemmer()\n",
    "stemmerS=SnowballStemmer('english')\n",
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'denied',\\\n",
    "            'died', 'agreed', 'owned', 'humbled', 'sized',\\\n",
    "            'meeting', 'stating', 'siezing', 'itemization',\\\n",
    "           'sensational', 'traditional', 'reference', 'colonizer',\\\n",
    "           'plotted']\n",
    "singles1=[stemmerP.stem(plural) for plural in plurals]\n",
    "singles2=[stemmerS.stem(plural) for plural in plurals]\n",
    "print singles1,'\\n',singles2\n",
    "singlesStr=','.join(singles1)   #list to str\n",
    "print singlesStr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
